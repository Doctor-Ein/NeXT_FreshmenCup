Now we provide an overview of the Transformer architecture in Fig. 11.7.1. At a high level, the Transformer encoder is a stack of multiple identical layers, where each layer has two sublayers (either is denoted as sublayer). The first is a multi-head self-attention pooling and the second is a positionwise feed-forward network. Specifically, in the encoder selfattention, queries, keys, and values are all from the outputs of the previous encoder layer. Inspired by the ResNet design of Section 8.6, a residual connection is employed around both sublayers. In the Transformer, for any input $\mathbf { x } \in \mathbb { R } ^ { d }$ at any position of the sequence, we require that sublayer $( \mathbf { x } ) \in \mathbb { R } ^ { d }$ so that the residual connection $\mathbf { x } + { \mathrm { s u b l a y e r } } ( \mathbf { x } ) \in \mathbb { R } ^ { d }$ is feasible. This addition from the residual connection is immediately followed by layer normalization (Ba et al., 2016). As a result, the Transformer encoder outputs a $d$ -dimensional vector representation for each position of the input sequence.
====================
