RAG模式：
你知道在《动手学深度学习》这本书的 注意力机制 这一章中，作者介绍了哪些注意力机制吗？回答不超过60个
字，
以下是RAG参考资料：
{"text": "In this chapter, we introduce attention models, starting with the most basic intuitions and the simplest instantiations of the idea. We then work our way up to the Transformer architecture, the vision Transformer, and the landscape of modern Transformer-based pretrained models.", "file_name": "unknown", "page": -1}
{"text": "Clearly we would not have introduced a simple database here if it wasn’t for the purpose of explaining deep learning. Indeed, this leads to one of the most exciting concepts introduced in deep learning in the past decade: the attention mechanism (Bahdanau et al., 2014). We will cover the specifics of its application to machine translation later. For now, simply consider the following: denote by ${ \\mathcal { D } } \\ { \\stackrel { \\mathrm { d e f } } { = } } \\ \\{ ( \\mathbf { k } _ { 1 } , \\mathbf { v } _ { 1 } ) , \\dots ( \\mathbf { k } _ { m } , \\mathbf { v } _ { m } ) \\}$ a database of $m$ tuples of keys and values. Moreover, denote by q a query. Then we can define the attention over $\\mathcal { D }$", "file_name": "unknown", "page": -1}
{"text": "The astute reader might wonder why we are providing this deep dive for a method that is over half a century old. First, it is one of the earliest precursors of modern attention mechanisms. Second, it is great for visualization. Third, and just as importantly, it demonstrates the limits of hand-crafted attention mechanisms. A much better strategy is to learn the mechanism, by learning the representations for queries and keys. This is what we will embark on in the following sections.", "file_name": "unknown", "page": -1}

====================
RAG模式：
你知道在《动手学深度学习》（元数据file_name为分区的就是）这本书的 注意力机制 这一章中，作者介绍了哪些注意力机制吗？回答不超过60个
字，
以下是RAG参考资料：
{"text": "Clearly we would not have introduced a simple database here if it wasn’t for the purpose of explaining deep learning. Indeed, this leads to one of the most exciting concepts introduced in deep learning in the past decade: the attention mechanism (Bahdanau et al., 2014). We will cover the specifics of its application to machine translation later. For now, simply consider the following: denote by ${ \\mathcal { D } } \\ { \\stackrel { \\mathrm { d e f } } { = } } \\ \\{ ( \\mathbf { k } _ { 1 } , \\mathbf { v } _ { 1 } ) , \\dots ( \\mathbf { k } _ { m } , \\mathbf { v } _ { m } ) \\}$ a database of $m$ tuples of keys and values. Moreover, denote by q a query. Then we can define the attention over $\\mathcal { D }$", "file_name": "unknown", "page": -1}
{"text": "Part 1: Basics and Preliminaries. Chapter 1 is an introduction to deep learning. Then, in Chapter 2, we quickly bring you up to speed on the prerequisites required for handson deep learning, such as how to store and manipulate data, and how to apply various numerical operations based on elementary concepts from linear algebra, calculus, and probability. Chapter 3 and Chapter 5 cover the most fundamental concepts and techniques in deep learning, including regression and classification; linear models; multilayer perceptrons; and overfitting and regularization.", "file_name": "unknown", "page": -1}
{"text": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].", "file_name": "unknown", "page": -1}

====================
