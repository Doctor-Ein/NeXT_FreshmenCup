import json
import os
import time
import sys

# import pyaudio
import boto3

# from amazon_transcribe.client import TranscribeStreamingClient
# from amazon_transcribe.handlers import TranscriptResultStreamHandler
# from amazon_transcribe.model import TranscriptEvent, TranscriptResultStream
from api_request_schema import api_request_list, get_model_ids

model_id = os.getenv('MODEL_ID', 'anthropic.claude-3-sonnet-20240229-v1:0') # ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ¨¡å‹id
aws_region = os.getenv('AWS_REGION', 'us-east-1') # ä»ç¯å¢ƒå˜é‡ä¸­è·å–AWSåŒºåŸŸ

if model_id not in get_model_ids(): # éªŒè¯æ¨¡å‹å­˜åœ¨äºé…ç½®æ¸…å•ä¸­
    print(f'Error: Models ID {model_id} in not a valid model ID. Set MODEL_ID env var to one of {get_model_ids()}.')
    sys.exit(0)

api_request = api_request_list[model_id] # å®šä¹‰å…¨å±€çš„api_requesté…ç½®è¡¨
config = {
    'log_level': 'info',  # One of: info, debug, none
    #'last_speech': "If you have any other questions, please don't hesitate to ask. Have a great day!",
    'region': aws_region,
    # å› ä¸ºè®¡åˆ’å°†pollyåŒ…æ‹¬transcribeéƒ½åˆ†ç¦»å‡ºå»
    # 'polly': {
    #     'Engine': 'neural',
    #     'LanguageCode': 'cmn-CN',
    #     'VoiceId': 'Zhiyu',
    #     'OutputFormat': 'pcm',
    # },
    'bedrock': {
        'api_request': api_request
    }
}

# ä¸¤ä¸ªè¾…åŠ©è°ƒè¯•çš„è¾“å‡ºæ—¥å¿—å‡½æ•°
def printInfo():
    """
    è¾“å‡ºç³»ç»Ÿä¿¡æ¯æ–‡æœ¬
    åŠŸèƒ½æè¿°ï¼šæ‰“å°æ”¯æŒçš„æ¨¡å‹ã€AWSåŒºåŸŸã€Amazon Bedrockæ¨¡å‹ã€Pollyé…ç½®å’Œæ—¥å¿—çº§åˆ«ç­‰ä¿¡æ¯
    """
    info_text = f'''
    *************************************************************
    [INFO] Supported FM models: {get_model_ids()}.
    [INFO] Change FM model by setting <MODEL_ID> environment variable. Example: export MODEL_ID=meta.llama2-70b-chat-v1

    [INFO] AWS Region: {config['region']}
    [INFO] Amazon Bedrock model: {config['bedrock']['api_request']['modelId']}
    [INFO] Polly config: engine {config['polly']['Engine']}, voice {config['polly']['VoiceId']}
    [INFO] Log level: {config['log_level']}

    [INFO] Hit ENTER to interrupt Amazon Bedrock. After you can continue speaking!
    [INFO] Go ahead with the voice chat with Amazon Bedrock!
    *************************************************************
    '''
    print(info_text) 
def printer(text, level):
    """
    æ‰“å°æ—¥å¿—ä¿¡æ¯
    åŠŸèƒ½æè¿°ï¼šæ ¹æ®æ—¥å¿—çº§åˆ«æ‰“å°ä¿¡æ¯
    :param text: è¦æ‰“å°çš„æ–‡æœ¬
    :param level: æ—¥å¿—çº§åˆ«ï¼ˆinfoæˆ–debugï¼‰
    """
    if config['log_level'] == 'info' and level == 'info':
        print(text)
    elif config['log_level'] == 'debug' and level in ['info', 'debug']:
        print(text)

# åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å’ŒAWSæœåŠ¡å®¢æˆ·ç«¯
bedrock_runtime = boto3.client(service_name='bedrock-runtime', region_name=config['region'])

class BedrockModelsWrapper:
    """
    Amazon Bedrockæ¨¡å‹å°è£…ç±»
    åŠŸèƒ½æè¿°ï¼šå®šä¹‰è¯·æ±‚ä½“ã€è·å–æµå—å’Œæ–‡æœ¬
    """

    @staticmethod
    def define_body(text, dialogue_list = [], images = []):
        """
        å®šä¹‰è¯·æ±‚ä½“
        åŠŸèƒ½æè¿°ï¼šæ ¹æ®ä¸åŒçš„æ¨¡å‹æä¾›è€…å®šä¹‰è¯·æ±‚ä½“
        :param text: è¾“å…¥æ–‡æœ¬
        :param dialogue_list: ä»å†å²ä¼šè¯ç®¡ç†åº“ä¸­è·å–çš„åˆ—è¡¨ï¼Œå…ƒç´ ä¸ºä¼šè¯å—å­—å…¸{role,content} -> è¿™é‡Œæ„æ€æ˜¯æŒ‡ä»ç‰¹æ€§ä¸Šè¯´ä¸æƒ³è¦åœ¨æŠŠå›¾ç‰‡æ’å…¥ï¼Œä½†æ˜¯æœ¬åœ°è¿˜æ˜¯è¦ä¿å­˜å›¾ç‰‡çš„ğŸ¤”
        :param images: è¾“å…¥å›¾ç‰‡çš„åˆ—è¡¨ï¼Œå…ƒç´ ä¸ºå­—å…¸{media_type, data}
        :return: è¯·æ±‚ä½“
        """
        model_id = config['bedrock']['api_request']['modelId']
        model_provider = model_id.split('.')[0]
        body = config['bedrock']['api_request']['body']

        # å®é™…ä¸Šæ­¤æ¬¡å¼€å‘åªéœ€è¦anthropic
        if model_provider == 'amazon':
            body['inputText'] = text
        elif model_provider == 'meta':
            if 'llama3' in model_id:
                body['prompt'] = f"""
                    <|begin_of_text|>
                    <|start_header_id|>user<|end_header_id|>
                    {text}, please output in Chinese.
                    <|eot_id|>
                    <|start_header_id|>assistant<|end_header_id|>
                    """
            else: 
                body['prompt'] = f"<s>[INST] {text}, please output in Chinese. [/INST]"
        elif model_provider == 'anthropic':
            if "claude-3" in model_id:
                # contentæ˜¯å•è½®å¯¹è¯çš„å†…å®¹,å¯ä»¥åŒ…å«textå’Œimage
                content = [
                    {
                        "type": "text",
                        "text": text
                    }
                ]
                # æ·»åŠ å›¾ç‰‡
                for image in images:
                    content.append({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image["media_type"],
                            "data": image["data"]
                        },
                    })
                
                body['messages'] = list(dialogue_list) # ä½¿ç”¨æµ…æ‹·ï¼Œé˜²æ­¢ä¸¤ä¸ªå¯¹è±¡ç›¸äº’å…³è”
                body['messages'].append({"role": "user", "content": content}) # æ·»åŠ è¿™ä¸€æ¬¡å¯¹è¯çš„content
            else: # è¿™ä¸ªçœ‹èµ·æ¥æ˜¯å¤„ç†anthropicçš„å…¶ä»–æ¨¡å‹çš„ï¼Œå°±ä¸ç”¨ç®¡å•¦ï¼ˆç¬‘ï¼‰
                body['prompt'] = f'\n\nHuman: {text}\n\nAssistant:'
        elif model_provider == 'cohere':
            body['prompt'] = text
        elif model_provider == 'mistral':
            body['prompt'] = f"<s>[INST] {text}, please output in Chinese. [/INST]"
        else:
            raise Exception('Unknown model provider.')

        return body

    @staticmethod
    def get_stream_chunk(event):
        """
        è·å–æµå—
        åŠŸèƒ½æè¿°ï¼šä»äº‹ä»¶ä¸­è·å–æµå—
        :param event: äº‹ä»¶å¯¹è±¡
        :return: æµå—
        """
        return event.get('chunk')

    @staticmethod
    def get_stream_text(chunk):
        """
        è·å–æµæ–‡æœ¬
        åŠŸèƒ½æè¿°ï¼šæ ¹æ®ä¸åŒçš„æ¨¡å‹æä¾›è€…ä»æµå—ä¸­è·å–æ–‡æœ¬
        :param chunk: æµå—
        :return: æ–‡æœ¬
        """
        model_id = config['bedrock']['api_request']['modelId']
        model_provider = model_id.split('.')[0]

        chunk_obj = ''
        text = ''
        if model_provider == 'amazon':
            chunk_obj = json.loads(chunk.get('bytes').decode())
            text = chunk_obj['outputText']
        elif model_provider == 'meta':
            chunk_obj = json.loads(chunk.get('bytes').decode())
            text = chunk_obj['generation']
        elif model_provider == 'anthropic':
            if "claude-3" in model_id:
                chunk_obj = json.loads(chunk.get('bytes').decode())
                if chunk_obj['type'] == 'message_delta':
                    print(f"\nStop reason: {chunk_obj['delta']['stop_reason']}")
                    print(f"Stop sequence: {chunk_obj['delta']['stop_sequence']}")
                    print(f"Output tokens: {chunk_obj['usage']['output_tokens']}")

                if chunk_obj['type'] == 'content_block_delta':
                    if chunk_obj['delta']['type'] == 'text_delta':
                        #print(chunk_obj['delta']['text'], end="")
                        text = chunk_obj['delta']['text']
            else: # Claude2.x
                chunk_obj = json.loads(chunk.get('bytes').decode())
                text = chunk_obj['completion']
        elif model_provider == 'cohere':
            chunk_obj = json.loads(chunk.get('bytes').decode())
            text = ' '.join([c["text"] for c in chunk_obj['generations']])
        elif model_provider == 'mistral':
            chunk_obj = json.loads(chunk.get('bytes').decode())
            text = chunk_obj['outputs'][0]['text']
        else:
            raise NotImplementedError('Unknown model provider.')

        printer(f'[DEBUG] {chunk_obj}', 'debug')
        return text

# çº¯å­—ç¬¦çº§çš„Bedrock_Streamæµå¤„ç†ç”Ÿæˆå™¨
from typing import Generator
def StreamHandler(bedrock_stream) -> Generator[str, None, None]:
    """
    å­—ç¬¦çº§æµå¼å¤„ç†å™¨
    åŠŸèƒ½ï¼š
    - ç›´æ¥é€å­—ç¬¦è¿”å›åŸå§‹æµå†…å®¹
    - ä¿æŒMarkdownç­‰ç»“æ„åŒ–æ–‡æœ¬å®Œæ•´æ€§
    - æœ€ä½å»¶è¿Ÿè¾“å‡º
    
    é€‚ç”¨äºï¼š
    - éœ€è¦å®æ—¶å­—ç¬¦çº§æ¸²æŸ“çš„åœºæ™¯
    - ä¿ç•™Markdown/ä»£ç ç­‰æ ¼å¼çš„åœºæ™¯
    """
    try:
        for event in bedrock_stream:
            if not (chunk := BedrockModelsWrapper.get_stream_chunk(event)):
                continue
                
            text = BedrockModelsWrapper.get_stream_text(chunk)
            if not text:
                continue

            yield text # ç›´æ¥è¿”å›ï¼Œå¤„ç†çš„æ­¥éª¤æå°‘
                
    except Exception as e:
        print(f"\n[Stream Error] {str(e)}")
        raise

class BedrockWrapper:
    """
    Amazon Bedrockå°è£…ç±»
    åŠŸèƒ½æè¿°ï¼šè°ƒç”¨Bedrockæ¨¡å‹å¹¶å¤„ç†å“åº”
    """

    def __init__(self):
        """
        åˆå§‹åŒ–Amazon Bedrockå°è£…ç±»
        """
        self.speaking = False

    def is_speaking(self):
        """
        æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¯´è¯
        :return: æ˜¯å¦æ­£åœ¨è¯´è¯
        """
        return self.speaking

    def invoke_bedrock(self, text, dialogue_list = [], images = []):
        """
        è°ƒç”¨Bedrockæ¨¡å‹
        åŠŸèƒ½æè¿°ï¼šè°ƒç”¨Bedrockæ¨¡å‹å¹¶å¤„ç†å“åº”
        :param text: è¾“å…¥æ–‡æœ¬
        :param data: æ•°æ®åˆ—è¡¨
        :param history: å†å²è®°å½•åˆ—è¡¨
        :return: è¾“å‡ºæ–‡æœ¬
        """
        printer('[DEBUG] Bedrock generation started', 'debug')
        self.speaking = True
        body = BedrockModelsWrapper.define_body(text, dialogue_list, images)
        printer(f"[DEBUG] Request body: {body}", 'debug')

        try:
            body_json = json.dumps(body)
            response = bedrock_runtime.invoke_model_with_response_stream( # åˆ©ç”¨boto3å®šä¹‰å“åº”å¯¹è±¡
                body=body_json,
                modelId=config['bedrock']['api_request']['modelId'],
                accept=config['bedrock']['api_request']['accept'],
                contentType=config['bedrock']['api_request']['contentType']
            )

            printer('[DEBUG] Capturing Bedrocks response/bedrock_stream', 'debug')
            bedrock_stream = response.get('body') # å®é™…çš„è°ƒç”¨å°±è¿™ä¸€å¥ï¼Œä½†æ˜¯å…¶å®æ˜¯åŒæ­¥é˜»å¡å¼çš„
            printer(f"[DEBUG] Bedrock_stream: {bedrock_stream}", 'debug')

            audio_gen = StreamHandler(bedrock_stream) # generator
            printer('[DEBUG] Created bedrock stream to audio generator', 'debug')

            response_text = '' # è®°å½•æ­¤æ¬¡å›å¤çš„å…¨éƒ¨æ–‡æœ¬
            for audio in audio_gen: # å¾—å…ˆä»generatorä¸­æ‰èƒ½è·å–æ–‡æœ¬
                print(audio,end="") # è°ƒè¯•ç”¨ï¼Œä¸è¦éšæ„æ¢è¡Œ
                response_text += audio
            print(history) # è¿™ä¸ªå¾—æ‹¿æ¥çœ‹çœ‹

        except Exception as e:
            print(e)
            time.sleep(2)
            self.speaking = False

        time.sleep(1)
        self.speaking = False
        printer('\n[DEBUG] Bedrock generation completed', 'debug')
        return response_text
    
if __name__ == '__main__':
    history = [] # å­˜å‚¨å¯¹è¯å†å²çš„åˆ—è¡¨
    bedrock_wrapper = BedrockWrapper() 
    while True:
        if not bedrock_wrapper.is_speaking():
            input_text = input("[Please Input]ï¼š")
            if len(input_text) != 0:
                request_text = input_text # è¿™é‡Œå¤„ç†æ¨¡å‹çš„é¢„è®¾æç¤ºè¯ï¼Ÿ
                printer(f'\n[INFO] request_text: {request_text}', 'info')

                return_output = bedrock_wrapper.invoke_bedrock(request_text, dialogue_list=history, images=[]) # ä¸ºäº†ä¸æ··ä¹±å¯¹è¯å†å²çš„é¡ºåºï¼Œä¸èƒ½å¼‚æ­¥è°ƒç”¨~

                history.append({"role":"user","content":[{ "type": "text","text": input_text}]}) # å‘å¯¹è¯å†å²ä¸­ï¼Œæ’å…¥ç”¨æˆ·è¾“å…¥ï¼ˆå–å‡ºäº†æç¤ºè¯ï¼‰
                history.append({"role":"assistant","content":[{ "type": "text","text": return_output}]}) # å‘å¯¹è¯å†å²ä¸­ï¼Œæ’å…¥æ¨¡å‹å›å¤
