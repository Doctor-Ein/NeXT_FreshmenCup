def process_content_list_docs(
    content_list_path: str,
    chunk_size: int = 300,
    chunk_overlap: int = 34,
) -> list:
    """
    1. è¯»å– MinerU å¯¼å‡ºçš„ content_list.json
    2. ä»¥æ¯ä¸ª layout-block ä¸ºå•å…ƒç”Ÿæˆåˆå§‹ chunkï¼Œåªä¿ç•™ type == 'text'
    3. å¯¹è¶…é•¿æ–‡æœ¬å†æ¬¡ç”¨ TokenTextSplitter è¿›è¡Œåˆ†å—
    4. è¿”å›åŒ…å« text å’Œ metadata çš„åˆ—è¡¨ï¼Œmetadata åŒ…æ‹¬ï¼š
       - file_name: ä»æ–‡ä»¶åæ¨æ–­
       - page: æ¥è‡ª page_idx
       - block_id: JSON ä¸­å—çš„ç´¢å¼•
       - chunk_id: block_id + chunk ç´¢å¼•
       - chunk_index: æœ¬èŠ‚ç‚¹ä¸­æ–‡æœ¬å—çš„ç´¢å¼•
    """
    from pathlib import Path
    import json
    from llama_index.core.text_splitter import TokenTextSplitter

    path = Path(content_list_path)
    if not path.exists() or not path.is_file():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° content_list.json: {content_list_path}")

    # è¯»å– JSON
    data = json.loads(path.read_text(encoding='utf-8'))
    print(f"ğŸ” è¯»å–åˆ° {len(data)} ä¸ªå¸ƒå±€å—")

    splitter = TokenTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separator=' ',
        backup_separators=['\n', '.', '?']
    )

    chunks = []
    file_name = path.stem.replace('_content_list', '')

    for block_id, block in enumerate(data):
        # ä»…ä¿ç•™çº¯æ–‡æœ¬å—
        if block.get('type') != 'text':
            continue

        raw_text = block.get('text', '').strip()
        if not raw_text:
            continue

        page = block.get('page_idx', 0)

        # æ ¹æ® token æ•°é‡å†³å®šæ˜¯å¦äºŒæ¬¡åˆ†å‰²
        subs = (
            splitter.split_text(raw_text)
            if len(raw_text.split()) > chunk_size
            else [raw_text]
        )

        for idx, sub in enumerate(subs):
            chunks.append({
                'text': sub,
                'metadata': {
                    'file_name': file_name,
                    'page': page,
                    'block_id': block_id,
                    'chunk_id': f"{block_id}_chunk_{idx}",
                    'chunk_index': idx
                }
            })

    print(f"âœ… ç”Ÿæˆ {len(chunks)} æ¡æœ‰æ•ˆæ–‡æœ¬ chunk")
    return chunks
