import json
from pathlib import Path

from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import MarkdownNodeParser
from llama_index.core.text_splitter import TokenTextSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.readers.file.markdown import MarkdownReader

from pymilvus import (
    connections,
    FieldSchema,
    CollectionSchema,
    DataType,
    Collection,
    utility
)

# -------------------- é…ç½®å‚æ•° --------------------
MARKDOWN_DIR = './Data/Paper/MinerU_Res/AlexNet'  # Markdown æ–‡ä»¶æ‰€åœ¨ç›®å½•
MILVUS_HOST = '0.0.0.0'
MILVUS_PORT = '19530'
COLLECTION_NAME = 'DL_KDB'  # Deep Learning Knowledge Database
LOCAL_MODEL_DIR = './local_models/bge-m3'  # æœ¬åœ° BGE-M3 æ¨¡å‹è·¯å¾„
VECTOR_DIM = 1024  # åµŒå…¥å‘é‡ç»´åº¦

# åˆå§‹åŒ–æœ¬åœ°åµŒå…¥æ¨¡å‹
embedding = HuggingFaceEmbedding(model_name=LOCAL_MODEL_DIR)


def process_markdown_docs(markdown_dir: str, chunk_size: int = 256, chunk_overlap: int = 48) -> list:
    """
    1. åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ Markdown æ–‡æ¡£
    2. æŒ‰ Markdown ç»“æ„åˆ†èŠ‚ç‚¹
    3. åŸºäº TokenTextSplitter è¿›è¡Œåˆ†å—
    4. è¿”å›åŒ…å« text å’Œ metadata çš„åˆ—è¡¨
    """
    # æ£€æŸ¥ç›®å½•
    path = Path(markdown_dir)
    if not path.exists() or not path.is_dir():
        raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {markdown_dir}")

    # åŠ è½½ Markdown æ–‡æ¡£
    reader = SimpleDirectoryReader(
        input_dir=markdown_dir,
        file_extractor={'.md': MarkdownReader()},
        required_exts=['.md'],
        exclude_hidden=True
    )
    documents = reader.load_data()
    print(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

    # æ‹†åˆ† Markdown èŠ‚ç‚¹
    md_parser = MarkdownNodeParser()
    nodes = md_parser.get_nodes_from_documents(documents)
    print(f"ç”Ÿæˆ {len(nodes)} ä¸ªèŠ‚ç‚¹")

    # æ–‡æœ¬åˆ†å—å™¨
    splitter = TokenTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separator=' ',
        backup_separators=['\n', '.', '?']
    )

    # å¤„ç†èŠ‚ç‚¹
    chunks = []
    for node in nodes:
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        cleaned = "\n".join([l.strip() for l in node.text.splitlines() if l.strip()])
        # åŸºæœ¬å…ƒæ•°æ®
        md = {
            'file_name': node.metadata.get('file_name', ''),
            'file_path': node.metadata.get('file_path', ''),
            'file_type': node.metadata.get('file_type', '')
        }
        # åˆ†å—å¹¶é™„åŠ å…ƒæ•°æ®
        for idx, txt in enumerate(splitter.split_text(cleaned)):
            chunks.append({
                'text': txt,
                'metadata': {
                    **md,
                    'chunk_id': f"{md['file_name']}_chunk_{idx}",
                    'chunk_index': idx
                }
            })

    return chunks


def create_milvus_collection(collection_name: str):
    """
    åˆ›å»ºæˆ–é‡ç½® Milvus é›†åˆï¼Œå­—æ®µåŒ…æ‹¬ id, text, metadata(JSON) å’Œ vector
    """
    # å¦‚æœå·²å­˜åœ¨åˆ™åˆ é™¤
    if utility.has_collection(collection_name):
        utility.drop_collection(collection_name)

    # å®šä¹‰ schema
    fields = [
        FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name='text', dtype=DataType.VARCHAR, max_length=4096),
        FieldSchema(name='metadata', dtype=DataType.JSON, nullable=True),
        FieldSchema(name='vector', dtype=DataType.FLOAT_VECTOR, dim=VECTOR_DIM)
    ]
    schema = CollectionSchema(fields=fields, description='Deep Learning Knowledge DB')

    # åˆ›å»ºé›†åˆ
    collection = Collection(name=collection_name, schema=schema, using='default')

    # åˆ›å»ºç´¢å¼•
    index_params = {
        'index_type': 'IVF_FLAT',
        'metric_type': 'L2',
        'params': {'nlist': 1024}
    }
    collection.create_index(field_name='vector', index_params=index_params)
    collection.load()
    return collection


def store_in_milvus(chunks: list):
    """
    å°†å¤„ç†åçš„æ–‡æœ¬å—è¿›è¡ŒåµŒå…¥å¹¶æ‰¹é‡æ’å…¥åˆ° Milvus
    """
    # è¿æ¥ Milvus
    connections.connect(alias='default', host=MILVUS_HOST, port=MILVUS_PORT)

    # åˆå§‹åŒ–é›†åˆ
    collection = create_milvus_collection(COLLECTION_NAME)

    # å‡†å¤‡æ•°æ®
    texts = []
    metas = []
    vecs = []
    for chunk in chunks:
        texts.append(chunk['text'])
        metas.append(chunk['metadata'])
        vecs.append(embedding.get_text_embedding(chunk['text']))

    # æ‰¹é‡æ’å…¥
    batch_size = 500
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_metas = metas[i:i+batch_size]
        batch_vecs = vecs[i:i+batch_size]
        collection.insert([batch_texts, batch_metas, batch_vecs])

    # æŒä¹…åŒ–å¹¶åŠ è½½
    # collection.flush()
    collection.load()
    print(f"ğŸš€ æˆåŠŸå­˜å‚¨ {len(texts)} æ¡è®°å½•åˆ° Milvus é›†åˆ '{COLLECTION_NAME}'")


if __name__ == '__main__':
    try:
        # 1. æ–‡æ¡£è§£æä¸åˆ†å—
        processed = process_markdown_docs(markdown_dir=MARKDOWN_DIR)
        # 2. å­˜å‚¨åˆ° Milvus
        store_in_milvus(processed)
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥: {e}")
